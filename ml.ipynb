{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-22T14:24:34.135822Z","iopub.status.busy":"2024-03-22T14:24:34.135417Z","iopub.status.idle":"2024-03-22T14:24:42.595809Z","shell.execute_reply":"2024-03-22T14:24:42.594900Z","shell.execute_reply.started":"2024-03-22T14:24:34.135782Z"},"trusted":true},"outputs":[],"source":["import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","import itertools\n","import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","from sklearn.metrics import classification_report, f1_score , confusion_matrix\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import Dense, Dropout , BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import layers,models,Model\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers.experimental import preprocessing\n","from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras import mixed_precision\n","mixed_precision.set_global_policy('mixed_float16')\n","\n","\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=UserWarning)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:24:42.598312Z","iopub.status.busy":"2024-03-22T14:24:42.597706Z","iopub.status.idle":"2024-03-22T14:24:50.061729Z","shell.execute_reply":"2024-03-22T14:24:50.060826Z","shell.execute_reply.started":"2024-03-22T14:24:42.598285Z"},"trusted":true},"outputs":[],"source":["lb = LabelEncoder()\n","dataset = {\n","             \"train_data\" : r'D:\\Train_data\\Train_data',\n","             \"valid_data\" : r'D:\\Validation_data',\n","          }\n","\n","all_data = []\n","for path in dataset.values():\n","    data = {\"imgpath\": [] , \"labels\": [] }\n","    category = os.listdir(path)\n","    for  index,folder in enumerate(category):\n","        folderpath = os.path.join(path, folder)\n","        filelist = os.listdir(folderpath)\n","        for file in filelist:\n","            fpath = os.path.join(folderpath, file)\n","            data[\"imgpath\"].append(fpath)\n","            data[\"labels\"].append(folder)\n","    all_data.append(data.copy())\n","    data.clear()\n","train_df = pd.DataFrame(all_data[0] , index=range(len(all_data[0]['imgpath'])))\n","valid_df = pd.DataFrame(all_data[1] , index=range(len(all_data[1]['imgpath'])))\n","\n","train_df['encoded_labels'] = lb.fit_transform(train_df['labels'])\n","valid_df['encoded_labels'] = lb.fit_transform(valid_df['labels'])\n","valid_df , test_df = train_test_split(valid_df ,  train_size= 0.95 , shuffle=True, random_state=124)\n","valid_df = valid_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:24:50.849236Z","iopub.status.busy":"2024-03-22T14:24:50.848865Z","iopub.status.idle":"2024-03-22T14:24:50.859563Z","shell.execute_reply":"2024-03-22T14:24:50.858419Z","shell.execute_reply.started":"2024-03-22T14:24:50.849206Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['Rosefinch',\n"," 'Kingfisher',\n"," 'Wagtail',\n"," 'Tailorbird',\n"," 'Myna',\n"," 'Crane',\n"," 'Crow',\n"," 'Egret',\n"," 'Peacock',\n"," 'Pitta']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train  = train_df[\"labels\"].value_counts()\n","label = train.tolist()\n","index = train.index.tolist()\n","index"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:24:50.861229Z","iopub.status.busy":"2024-03-22T14:24:50.860886Z","iopub.status.idle":"2024-03-22T14:24:50.880026Z","shell.execute_reply":"2024-03-22T14:24:50.878978Z","shell.execute_reply.started":"2024-03-22T14:24:50.861203Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------Train-------------\n","                                             imgpath     labels\n","0  D:\\kct\\Train_data\\Train_data/Comm...  Rosefinch\n","1  D:\\kct\\Train_data\\Train_data/Comm...  Rosefinch\n","2  D:\\kct\\Train_data\\Train_data/Comm...  Rosefinch\n","3  D:\\kct\\Train_data\\Train_data/Comm...  Rosefinch\n","4  D:\\kct\\Train_data\\Train_data/Comm...  Rosefinch\n","(8000, 3)\n","--------Validation----------\n","                                             imgpath      labels\n","0  D:\\kct\\Validation_data/Catt...       Egret\n","1  D:\\kct\\Validation_data/Comm...  Tailorbird\n","2  D:\\kct\\Validation_data/Whit...     Wagtail\n","3  D:\\kct\\Validation_data/Catt...       Egret\n","4  D:\\kct\\Validation_data/Whit...  Kingfisher\n","(1900, 3)\n","----------Test--------------\n","                                             imgpath     labels\n","0  D:\\kct\\Validation_data/Comm...  Rosefinch\n","1  D:\\kct\\Validation_data/Indi...      Pitta\n","2  D:\\kct\\Validation_data/Saru...      Crane\n","3  D:\\kct\\Validation_data/Indi...      Pitta\n","4  D:\\kct\\Validation_data/Saru...      Crane\n","(100, 3)\n"]}],"source":["print(\"----------Train-------------\")\n","print(train_df[[\"imgpath\", \"labels\"]].head(5))\n","print(train_df.shape)\n","print(\"--------Validation----------\")\n","print(valid_df[[\"imgpath\", \"labels\"]].head(5))\n","print(valid_df.shape)\n","print(\"----------Test--------------\")\n","print(test_df[[\"imgpath\", \"labels\"]].head(5))\n","print(test_df.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:24:50.881524Z","iopub.status.busy":"2024-03-22T14:24:50.881247Z","iopub.status.idle":"2024-03-22T14:25:11.194596Z","shell.execute_reply":"2024-03-22T14:25:11.193650Z","shell.execute_reply.started":"2024-03-22T14:24:50.881499Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 15000 validated image filenames belonging to 10 classes.\n","Found 2850 validated image filenames belonging to 10 classes.\n","Found 150 validated image filenames belonging to 10 classes.\n","CPU times: user 290 ms, sys: 334 ms, total: 624 ms\n","Wall time: 20.3 s\n"]}],"source":["%%time\n","\n","BATCH_SIZE = 50\n","IMAGE_SIZE = (224, 224)\n","\n","\n","generator = ImageDataGenerator(\n","    preprocessing_function = tf.keras.applications.efficientnet.preprocess_input,\n",")\n","\n","train_images = generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='imgpath',\n","    y_col='labels',\n","    target_size=IMAGE_SIZE,\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    seed=42,\n",")\n","\n","val_images = generator.flow_from_dataframe(\n","    dataframe=valid_df,\n","    x_col='imgpath',\n","    y_col='labels',\n","    target_size=IMAGE_SIZE,\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    shuffle=False\n",")\n","\n","test_images = generator.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='imgpath',\n","    y_col='labels',\n","    target_size=IMAGE_SIZE,\n","    color_mode='rgb',\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    shuffle=False\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:25:11.196252Z","iopub.status.busy":"2024-03-22T14:25:11.195937Z","iopub.status.idle":"2024-03-22T14:25:29.706195Z","shell.execute_reply":"2024-03-22T14:25:29.705310Z","shell.execute_reply.started":"2024-03-22T14:25:11.196224Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-l_notop.h5\n","473176280/473176280 [==============================] - 3s 0us/step\n"]}],"source":["pretrained_model =tf.keras.applications.efficientnet_v2.EfficientNetV2L(\n","    input_shape=(224, 224, 3),\n","    include_top=False, \n","    weights='None',\n","    pooling='max',\n","    classes=10\n",")\n","\n","for i, layer in enumerate(pretrained_model.layers):\n","    pretrained_model.layers[i].trainable = False"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:25:29.707728Z","iopub.status.busy":"2024-03-22T14:25:29.707418Z","iopub.status.idle":"2024-03-22T14:25:35.005895Z","shell.execute_reply":"2024-03-22T14:25:35.005043Z","shell.execute_reply.started":"2024-03-22T14:25:29.707699Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," inputLayer (InputLayer)     [(None, 224, 224, 3)]     0         \n","                                                                 \n"," AugmentationLayer (Sequenti  (None, 224, 224, 3)      0         \n"," al)                                                             \n","                                                                 \n"," efficientnetv2-l (Functiona  (None, 1280)             117746848 \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 256)               327936    \n","                                                                 \n"," activation (Activation)     (None, 256)               0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 256)              1024      \n"," ormalization)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                2570      \n","                                                                 \n"," activationLayer (Activation  (None, 10)               0         \n"," )                                                               \n","                                                                 \n","=================================================================\n","Total params: 118,078,378\n","Trainable params: 331,018\n","Non-trainable params: 117,747,360\n","_________________________________________________________________\n","None\n","\n"]}],"source":["num_classes = len(set(train_images.classes))\n","\n","\n","# Data Augmentation Step\n","augment = tf.keras.Sequential([\n","  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n","  layers.experimental.preprocessing.RandomRotation(0.1),\n","  layers.experimental.preprocessing.RandomZoom(0.1),\n","  layers.experimental.preprocessing.RandomContrast(0.1),\n","], name='AugmentationLayer')\n","\n","\n","\n","inputs = layers.Input(shape = (224,224,3), name='inputLayer')\n","x = augment(inputs)\n","pretrain_out = pretrained_model(x, training = False)\n","x = layers.Dense(256)(pretrain_out)\n","x = layers.Activation(activation=\"relu\")(x) \n","x = BatchNormalization()(x)\n","x = layers.Dropout(0.3)(x)\n","x = layers.Dense(num_classes)(x)\n","\n","outputs = layers.Activation(activation=\"softmax\", dtype=tf.float32, name='activationLayer')(x) # mixed_precision need separated Dense and Activation layers\n","model = Model(inputs=inputs, outputs=outputs)\n","\n","\n","\n","model.compile(\n","    optimizer=Adam(0.0005),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","print(model.summary())\n","serialized_model = tf.keras.layers.serialize(model)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:25:35.009833Z","iopub.status.busy":"2024-03-22T14:25:35.009545Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n","300/300 [==============================] - 243s 693ms/step - loss: 0.4974 - accuracy: 0.8453 - val_loss: 0.3107 - val_accuracy: 0.9053 - lr: 5.0000e-04\n","Epoch 2/25\n","300/300 [==============================] - 123s 410ms/step - loss: 0.2673 - accuracy: 0.9139 - val_loss: 0.2197 - val_accuracy: 0.9305 - lr: 5.0000e-04\n","Epoch 3/25\n","300/300 [==============================] - 124s 412ms/step - loss: 0.2381 - accuracy: 0.9263 - val_loss: 0.1789 - val_accuracy: 0.9456 - lr: 5.0000e-04\n","Epoch 4/25\n","300/300 [==============================] - 126s 421ms/step - loss: 0.2219 - accuracy: 0.9318 - val_loss: 0.1597 - val_accuracy: 0.9512 - lr: 5.0000e-04\n","Epoch 5/25\n","300/300 [==============================] - 125s 415ms/step - loss: 0.2008 - accuracy: 0.9381 - val_loss: 0.1658 - val_accuracy: 0.9442 - lr: 5.0000e-04\n","Epoch 6/25\n","300/300 [==============================] - 121s 403ms/step - loss: 0.1981 - accuracy: 0.9356 - val_loss: 0.1645 - val_accuracy: 0.9519 - lr: 5.0000e-04\n","Epoch 7/25\n","300/300 [==============================] - 124s 412ms/step - loss: 0.1725 - accuracy: 0.9462 - val_loss: 0.1370 - val_accuracy: 0.9582 - lr: 1.0000e-04\n","Epoch 8/25\n","300/300 [==============================] - 126s 418ms/step - loss: 0.1577 - accuracy: 0.9501 - val_loss: 0.1267 - val_accuracy: 0.9604 - lr: 1.0000e-04\n","Epoch 9/25\n","300/300 [==============================] - 125s 417ms/step - loss: 0.1563 - accuracy: 0.9492 - val_loss: 0.1230 - val_accuracy: 0.9614 - lr: 1.0000e-04\n","Epoch 10/25\n","300/300 [==============================] - 126s 421ms/step - loss: 0.1541 - accuracy: 0.9515 - val_loss: 0.1235 - val_accuracy: 0.9582 - lr: 1.0000e-04\n","Epoch 11/25\n","300/300 [==============================] - 127s 422ms/step - loss: 0.1505 - accuracy: 0.9515 - val_loss: 0.1198 - val_accuracy: 0.9632 - lr: 1.0000e-04\n","Epoch 12/25\n","300/300 [==============================] - 124s 412ms/step - loss: 0.1441 - accuracy: 0.9551 - val_loss: 0.1187 - val_accuracy: 0.9632 - lr: 1.0000e-04\n","Epoch 13/25\n","300/300 [==============================] - 127s 421ms/step - loss: 0.1426 - accuracy: 0.9557 - val_loss: 0.1137 - val_accuracy: 0.9632 - lr: 1.0000e-04\n","Epoch 14/25\n","300/300 [==============================] - 125s 417ms/step - loss: 0.1444 - accuracy: 0.9545 - val_loss: 0.1176 - val_accuracy: 0.9639 - lr: 1.0000e-04\n","Epoch 15/25\n","300/300 [==============================] - 126s 418ms/step - loss: 0.1397 - accuracy: 0.9553 - val_loss: 0.1154 - val_accuracy: 0.9639 - lr: 1.0000e-04\n","Epoch 16/25\n","300/300 [==============================] - 125s 416ms/step - loss: 0.1337 - accuracy: 0.9556 - val_loss: 0.1021 - val_accuracy: 0.9677 - lr: 2.0000e-05\n","Epoch 17/25\n","300/300 [==============================] - 127s 423ms/step - loss: 0.1365 - accuracy: 0.9559 - val_loss: 0.1020 - val_accuracy: 0.9667 - lr: 2.0000e-05\n","Epoch 18/25\n","300/300 [==============================] - 129s 430ms/step - loss: 0.1352 - accuracy: 0.9577 - val_loss: 0.1019 - val_accuracy: 0.9670 - lr: 2.0000e-05\n","Epoch 19/25\n","300/300 [==============================] - 127s 422ms/step - loss: 0.1346 - accuracy: 0.9565 - val_loss: 0.0996 - val_accuracy: 0.9677 - lr: 2.0000e-05\n","Epoch 20/25\n","300/300 [==============================] - 132s 438ms/step - loss: 0.1340 - accuracy: 0.9577 - val_loss: 0.0994 - val_accuracy: 0.9688 - lr: 2.0000e-05\n","Epoch 21/25\n","300/300 [==============================] - 129s 429ms/step - loss: 0.1259 - accuracy: 0.9587 - val_loss: 0.1013 - val_accuracy: 0.9688 - lr: 2.0000e-05\n","Epoch 22/25\n","300/300 [==============================] - 133s 443ms/step - loss: 0.1303 - accuracy: 0.9591 - val_loss: 0.0984 - val_accuracy: 0.9705 - lr: 2.0000e-05\n","Epoch 23/25\n","300/300 [==============================] - 138s 460ms/step - loss: 0.1329 - accuracy: 0.9556 - val_loss: 0.0978 - val_accuracy: 0.9702 - lr: 2.0000e-05\n","Epoch 24/25\n","300/300 [==============================] - 129s 430ms/step - loss: 0.1260 - accuracy: 0.9580 - val_loss: 0.0979 - val_accuracy: 0.9695 - lr: 2.0000e-05\n","Epoch 25/25\n","300/300 [==============================] - 133s 442ms/step - loss: 0.1273 - accuracy: 0.9581 - val_loss: 0.0972 - val_accuracy: 0.9705 - lr: 2.0000e-05\n","\n"]}],"source":["history = model.fit(\n","    train_images,\n","    steps_per_epoch=len(train_images),\n","    validation_data=val_images,\n","    validation_steps=len(val_images),\n","    epochs=30,\n","    callbacks=[\n","        EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n","                               patience = 3,\n","                               restore_best_weights = True), \n","        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min') \n","    ]\n",")\n"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input_1 True\n","rescaling True\n","stem_conv True\n","stem_bn False\n","stem_activation True\n","block1a_project_conv True\n","block1a_project_bn False\n","block1a_project_activation True\n","block1a_add True\n","block1b_project_conv True\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," inputLayer (InputLayer)     [(None, 224, 224, 3)]     0         \n","                                                                 \n"," AugmentationLayer (Sequenti  (None, None, None, None)  0        \n"," al)                                                             \n","                                                                 \n"," efficientnetv2-l (Functiona  (None, 1280)             117746848 \n"," l)                                                              \n","                                                                 \n"," dense (Dense)               (None, 256)               327936    \n","                                                                 \n"," activation (Activation)     (None, 256)               0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 256)              1024      \n"," ormalization)                                                   \n","                                                                 \n"," dropout (Dropout)           (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                2570      \n","                                                                 \n"," activationLayer (Activation  (None, 10)               0         \n"," )                                                               \n","                                                                 \n","=================================================================\n","Total params: 118,078,378\n","Trainable params: 117,052,714\n","Non-trainable params: 1,025,664\n","_________________________________________________________________\n","None\n","Epoch 1/30\n","300/300 [==============================] - 445s 970ms/step - loss: 0.0642 - accuracy: 0.9798 - val_loss: 0.0295 - val_accuracy: 0.9912 - lr: 1.0000e-05\n","Epoch 2/30\n","300/300 [==============================] - 284s 944ms/step - loss: 0.0244 - accuracy: 0.9935 - val_loss: 0.0134 - val_accuracy: 0.9958 - lr: 1.0000e-05\n","Epoch 3/30\n","300/300 [==============================] - 283s 942ms/step - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.0069 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 4/30\n","300/300 [==============================] - 282s 940ms/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.0086 - val_accuracy: 0.9982 - lr: 1.0000e-05\n","Epoch 5/30\n","300/300 [==============================] - 282s 939ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0082 - val_accuracy: 0.9975 - lr: 1.0000e-05\n","Epoch 8/30\n","300/300 [==============================] - 282s 939ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0059 - val_accuracy: 0.9982 - lr: 2.0000e-06\n","Epoch 9/30\n","300/300 [==============================] - 284s 947ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0047 - val_accuracy: 0.9982 - lr: 2.0000e-06\n","Epoch 10/30\n","300/300 [==============================] - 283s 941ms/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0050 - val_accuracy: 0.9982 - lr: 4.0000e-07\n","\n"]}],"source":["pretrained_model.trainable = True\n","for layer in pretrained_model.layers:\n","    if isinstance(layer, layers.BatchNormalization): # set BatchNorm layers as not trainable\n","        layer.trainable = False\n","        \n","for l in pretrained_model.layers[:10]:\n","    print(l.name, l.trainable)\n","\n","model.compile(\n","    optimizer=Adam(0.00001), # fine tuning requires very little learning rate\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","print(model.summary())\n","history = model.fit(\n","    train_images,\n","    steps_per_epoch=len(train_images),\n","    validation_data=val_images,\n","    validation_steps=len(val_images),\n","    epochs=30,\n","    callbacks=[\n","        EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n","                               patience = 5,\n","                               restore_best_weights = True), # if val loss decreases for 5 epochs in a row, stop training,\n","        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min') \n","    ]\n",")\n"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 0.00122\n","Test Accuracy: 99.99%\n","Test Precision: .99998\n","Test Recall: .99997\n","F1 Score: .99989\n","\n"]}],"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Evaluate the model\n","results = model.evaluate(test_images, verbose=0)\n","\n","# Compute predictions\n","y_pred = model.predict(test_images)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","# Compute true labels\n","true_labels = [np.argmax(label) for label in test_labels]\n","\n","# Compute precision, recall, and F1-score\n","precision = precision_score(true_labels, y_pred_classes, average='weighted')\n","recall = recall_score(true_labels, y_pred_classes, average='weighted')\n","f1 = f1_score(true_labels, y_pred_classes, average='weighted')\n","\n","# Print evaluation metrics\n","print(\"    Test Loss: {:.5f}\".format(results[0]))\n","print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n","print(\" Test Precision: {:.5f}\".format(precision))\n","print(\"    Test Recall: {:.5f}\".format(recall))\n","print(\"      F1 Score: {:.5f}\".format(f1))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing import image\n","\n","# Load the image\n","img_path = ''  \n","img = image.load_img(img_path, target_size=(224, 224))  \n","img_array = image.img_to_array(img)\n","img_array = np.expand_dims(img_array, axis=0)  \n","prediction = model.predict(img_array)\n","predicted_class = np.argmax(prediction, axis=1)[0]\n","print(predicted_class)\n","class_indices = [0,1,2,3,4,5,6,7,8,9]\n","class_names = ['Rosefinch', 'Kingfisher', 'Wagtail', 'Tailorbird', 'Myna', 'Crane', 'Crow', 'Egret', 'Peacock', 'Pitta']\n","predicted_class_name = class_names[predicted_class]\n","\n","print(\"Predicted Class:\", predicted_class_name)\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3027308,"sourceId":5205289,"sourceType":"datasetVersion"},{"datasetId":4642315,"sourceId":7903833,"sourceType":"datasetVersion"}],"dockerImageVersionId":30528,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
